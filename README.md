# our_awesome_project

## Проект 5: Классификация промоторных последовательностей

**Задача:** Разработать классификатор на основе трансформера для промоторных последовательностей от непромоторных геномных последовательностей.

**Обоснование:** Фундаментальная проблема регуляции генов. Это задача бинарной классификации, которая учит студентов работе с регуляторными мотивами. Можно анализировать веса внимания модели, чтобы увидеть, каким частям последовательности модель “внимает” при принятии решения.

**Ожидаемый результат:** Бинарный классификатор и анализ важных признаков последовательности с помощью карт внимания.

**Данные и Инструменты:**
- **Наборы данных:** `Eukaryotic Promoter Database (EPD)`. Негативные последовательности можно брать из случайных участков генома. 
- **Предобученные модели:** `DNABERT`.
- **Инструменты:** Python, библиотека transformers (для `DNABERT`).

## ХОД РАБОТЫ:

### 1 ЭТАП. Загрузка данных

Скачаем последовательности из Eukaryotic Promoter Database (EPD).

Негативные примеры (не-промоторы): Сгенерируем или скачаем случайные участки генома того же организма и той же длины, что и промоторы из EPD.

### 2 ЭТАП. Очистка и предобработка данных

Убедимся, что все последовательности имеют одинаковую длину. Если нет — обрезаем до минимальной или дополняем до максимальной, удаляем "нестандартные".

Проверяем на дубликаты, чтобы модель не переобучалась на одних и тех же примерах.

Преобразуем текстовые последовательности ДНК (из букв A, T, G, C) в числовую форму, понятную модели. (A = [1,0,0,0], T = [0,1,0,0], G = [0,0,1,0], C = [0,0,0,1]).

### 3 ЭТАП. Извлечение признаков

Простые признаки: Частота каждого нуклеотида, частота динуклеотидов (AA, AT, AC, ...), GC-состав.

Сложные/Специфичные признаки: Наличие известных промоторных мотивов (например, TATA-box, INR), которые можно искать с помощью шаблонов (PSSM).

При этом будем использовать предобученную модель (DNABERT): пропускаем последовательности через DNABERT и берем эмбеддинги (векторные представления) из последнего или предпоследнего слоя. Эти эмбеддинги и будут нашими "признаками" — они уже содержат в сжатом виде контекстную информацию о последовательности.

### 4 ЭТАП. Проба линейных моделей

Тестируем простые, быстрые и интерпретируемые моделей. Это создаст "бейзлайн" (базовый уровень качества), который нужно превзойти более сложными моделями.

Модели:

1. Главные компоненты (PCA, метод уменьшения размерности). Его используем перед подачей данных в модель (например, в SVM), чтобы убрать шум и ускорить обучение.

2. Logistic Regression: стартовая модель, хорошо работает если данные примерно линейно разделимы.

3. SVM (Support Vector Machine). Классическая и мощная модель для классификации. Хорошо работает на данных разной природы.

Посмотрим на метрики качества (Accuracy, Precision, Recall, F1-score) и сделаем вывод о бейзлайне.

### 5 ЭТАП. Проба нелинейных моделей

Тестируем более сложные модели, которые могут улавливать неочевидные, нелинейные зависимости в данных. Цель — улучшить результат бейзлайна.

Модели:

1. Решающие деревья (Decision Trees): Простые нелинейные модели, которые легко интерпретировать.

2. Ансамбли деревьев (Random Forest, XGBoost): Очень часто показывают лучшие результаты на небольших датасетах.

3. Нейросети (Neural Networks): Многослойный перцептрон (MLP). Может хорошо работать на извлеченных признаках (частотах, эмбеддингах).

4. Гауссовы процессы (Gaussian Processes): Более экзотическая и требовательная к ресурсам модель, которая дает не только предсказание, но и оценку uncertainty (неопределенности).

### 6 ЭТАП. DNADERT

Тонкая настройка DNABERT (Fine-tuning): берем предобученную модель DNABERT и дообучаем ее на наших данных о промоторах. 

После этого сможем не только классифицировать, но и использовать анализ весов внимания, чтобы посмотреть, какие именно нуклеотиды модель считает важными для принятия решения.


## Авторы проекта: Третьякова Юлия Б06-303, Полушина Дарья Б06-303, Мавричева Анна Б06-303
